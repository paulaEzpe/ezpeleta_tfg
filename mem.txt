EJECUCION:
    - para ejecutar el backend:
    dentro de la carpeta backend: python3 server.py 

    - para ejecutar el frontend:
    dentro de la carpeta de frontend: npm start

    - para ejecutar lo de opensearch:
    dentro de la carpeta de opensearch: python3 high_client.py

NOTAS:
primero hacer el frontend con react-bootstrap
luego hacer el backend en python con venv y flask
luego usar mongoDB para intentar meter jsons en la bd 
los 49jsons de la bd se metieron bien pero no iban a caber todos
del resto porque eran en total 50gb sin descomprimir lo q lo haria unos 250gb y mondodb atlas ofrece 512mb.
se hizo una prueba de carga y se comprobo que cabian unos 2600 jsons. nada que ver con todos los que habia que cargar
por tanto se paso a utilizar otra bd. se barajó tambien con cassandra poque tb almacena jsons, pero siendo que a parte de almacenarlos tb hay que indexafrlos para buscar rapidamente entre tanto documento, se paso a elegir opensearch. con docker, ya que es la opcion que recomiendan ellos.
me daba error con el docker el opensearch ultima version de 2.10... asi que busque por internet y era porque no era estabble. use la 1.2.3 y luego me dio un error que se resolvio? con pip install 'urllib3<2'.
problema al almacenar campos por el limite de los index, porque al almacenar 10 documentos, ya son un total de 20000 campos.
ya he arreglado eso y lo que he hecho ha sido serializar el body entero, de manera que el indice queda
(paper_id, documento_serializado_entero). los tiempos que de momento manejo son los siguientes:

tiempo de inserción de la carpeta_08 con 41 elementos, 122,4 MB:
--- 59.84279656410217 seconds ---
--- 778 insertados ---
Además, esto ocupa 182,2 mb en opensearch

segun ese dato, el calculo para el tiempo de insercion de la carpeta con mas documentos, que tiene 1275 elementos y 13.3 GB:
122 ---- 1 minuto
13300 ---- 109 minutos
33600 ----- 300 minutos aprox

para decidir si borrar e campo ref_entries de los jsons, ya que es un campo con el que nunca se va a trabajar y puede merecer la pena 
borrarlo en cuanto a espacio "salvado" en opensearch, se ha extraido el campo de ref_entries de 778 documentos jsons
y se han ido insertando el un archivo de texto. el tamaño de ese archivo es  333202 bytes, que entre los 778 documentos
hace una media de 428.28020565552697 por documento. 

ademas, hay que explicar lo de los autores que a veces en las referencias no salen en su propio campo
sino que pueden aparecer en el nombre de la referencia. mirar ademas si el nombre de los autores tiene algun id o algo (no tienen id, en el campo authors_parsed solo 
aparecen los nombres y apellidos). y por que eliminar el campo de ref_entries?:

    Tamaño total de la base de datos: Si la base de datos es pequeña o moderada, una reducción de 400 bytes por archivo podría no parecer significativa en términos absolutos. Sin embargo, en bases de datos más grandes, esta reducción puede sumar y tener un impacto considerable en el tamaño total de la base de datos.

    Recursos del sistema: Si la base de datos se encuentra en un entorno con recursos limitados, como espacio en disco o ancho de banda, cualquier reducción en el tamaño de los archivos puede ser beneficiosa para optimizar el rendimiento y la eficiencia del sistema.

    Velocidad de acceso y transferencia: Reducir el tamaño de los archivos puede mejorar la velocidad de acceso y transferencia de datos, especialmente en situaciones donde los recursos de red son limitados o el acceso a la base de datos se realiza a través de conexiones lentas.

    Facilidad de mantenimiento: Menos datos pueden significar una menor complejidad y un mantenimiento más fácil de la base de datos. Esto puede ser particularmente útil si necesitas realizar copias de seguridad regulares de la base de datos o si estás gestionando la base de datos en un entorno de almacenamiento en la nube donde el costo está relacionado con la cantidad de datos almacenados.

se ha decidido que se indexará por autores, paper_id, disciplina, categoría y título

Data inserted successfully.
--- 123.13113975524902 seconds ---
--- 1327 insertados ---

Data inserted successfully.
--- 178.07762551307678 seconds ---
--- 1596 insertados ---


estaba teniendo problemas al buscar titulos porque buscaba un titulo que tecnicawmtne no estaba insertado y me salian muchos id, pero lo que pasaba
era que estaba usando match en vez de term, que mira si coincide una parte de la cadena.

--- 1639.703936100006 seconds ---
--- 15827 insertados ---

Data inserted successfully.
--- 6170.454500436783 seconds ---
--- 58385 insertados --- esto la carpeta de 11.9 gb (100 minutos, casi 2h)

Data inserted successfully.
--- 10190.409915924072 seconds ---
--- 65757 insertados ---



--------------------------------------------------------------------------------
 18/04: escribir en la memoria:
 implementacion y diseño del frontend
 implementacion y diseño del backend
    implementacion de los distintos modulos???
    dentro del de texto, los algoritmos para comprobar si una cita esta contenida en un parrafo y la comparacion de tiempos de estos???
    toda la parte que estoy haciendo ahora de entrenar el modelo para saber a partir de una cita y una referencia si te vale con leerte el titulo y abstract o no¿¿¿

all words: https://www.academicvocabulary.info/download.asp


se han usado estos modulos:
pip3 install transformers --break-system-packages
pip3 install torch --break-system-packages
pip3 install sentence_transformers --break-system-packages


El tiempo de entrenamiento de un modelo de Word2Vec no escala de manera lineal con el aumento de la dimensionalidad del vector. Sin embargo, hay algunas consideraciones generales que pueden ayudarte a estimar el tiempo de entrenamiento.

    Complejidad Computacional: La complejidad computacional de entrenar un modelo de Word2Vec aumenta con la dimensionalidad del vector. Específicamente, el tiempo de entrenamiento puede crecer de manera cuadrática con la dimensionalidad en el peor de los casos, aunque esto puede variar dependiendo de la implementación y las optimizaciones específicas del algoritmo.

    Recursos Computacionales: La cantidad de recursos computacionales disponibles, como la CPU, la memoria y la capacidad de paralelización, también afecta significativamente el tiempo de entrenamiento.

    Tamaño del Corpus: El tamaño del corpus de entrenamiento también es un factor importante. Un corpus más grande requerirá más tiempo de procesamiento.

Estimación Aproximada

Dado que mencionaste que entrenar un modelo con una dimensionalidad de 50 tomó 89 horas, y asumiendo una relación cuadrática simplificada (aunque en la práctica puede variar), puedes usar la siguiente fórmula para una estimación aproximada:

Tiempo nuevo≈Tiempo original×(Nueva dimensionalidadDimensionalidad original)2Tiempo nuevo≈Tiempo original×(Dimensionalidad originalNueva dimensionalidad​)2

En tu caso:

Tiempo nuevo≈89 horas×(20050)2Tiempo nuevo≈89 horas×(50200​)2
Tiempo nuevo≈89 horas×(4)2Tiempo nuevo≈89 horas×(4)2
Tiempo nuevo≈89 horas×16Tiempo nuevo≈89 horas×16
Tiempo nuevo≈1424 horasTiempo nuevo≈1424 horas
Consideraciones Finales

    Optimización: Hay optimizaciones posibles que pueden reducir significativamente el tiempo de entrenamiento, como el uso de técnicas de sampling negativo, ventanas de contexto más pequeñas, o hardware más potente.

    Paralelización: Asegúrate de usar la capacidad de paralelización de Word2Vec (como el parámetro workers en Gensim) para aprovechar múltiples núcleos de CPU.

    Validación Práctica: La fórmula proporcionada es una estimación teórica. En la práctica, te recomiendo realizar una prueba con una dimensionalidad intermedia para obtener una mejor estimación basada en tu configuración específica.