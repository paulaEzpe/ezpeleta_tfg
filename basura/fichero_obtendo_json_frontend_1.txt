language is a natural interface for systems like robots or personal assistants that interact with human users one way to interpret language in these interactive settings is to train an instruction following agent a model that learns to map commands like go three steps forward to the door to a sequence of actions in context eg cite cite inter alia instructions describe how an agent should act in an immediate context but to build models that can generalizecarrying out a users goals in new contexts and learning user preferences over repeated interactionsagents should also infer why actions are taken grounding language to reward functions extends the standard instruction following setup in this way representing the goals and preferences that underlie actions and allowing agents to autonomously carry out correct actions in new contexts eg cite figure

however when people interact with systems they often primarily aim to achieve specific tasks rather than literally describing their preferences in full how do we infer general goals and preferences from utterances in these settings consider a flight booking agent like the one in figure ref by inferring the users reward function indicating their preference for carrier price and other flight features beyond just selecting the right flight such a system would be able to autonomously book flights on behalf of the user in other instances to do so the system might use the actions the user commands as evidence about what they prefer recovering rewards from actions using languagefree techniques like inverse reinforcement learning irl cite for example the system can select a flight the user might like in a new instance by matching features from their past flight bookings

the key idea of our work is that the way that a user refers to their desired actions with language also reveals important information about their reward the fact that they said the jetblue flight and not the expensive flight conveys what matters to them intuitively in settings with repeated interactions utterances are optimized to communicate information that is generalizableimplicitly helping listeners make useful inferences for acting on a longer horizon we implement this idea with a pragmatic model of how speakers humans generate such language speakers choose utterances that both elicit rewardmaximizing actions in a particular context and faithfully describe the reward given an utterance our model infers that the most likely rewards are the ones that would have made a speaker likely to choose that utterance

to evaluate our model we construct and release a dataset for mapping language to rewards flightpref containing natural language utterances from humans with underlying preferences humans interact in a multiturn flight booking game similar to figure ref where we provide a user player with a reward function representing flight preferences the goal of the game is for the user to communicate these preferences in natural language to an assistant player who is tasked with booking preferred flights for the user we present this dataset as a challenging benchmark for reward learning from language and interaction

in our experiments we show that our model can infer reward functions from natural language improve reward estimates consistently over repeated interactions and use inferred rewards to accurately select optimal actions in heldout environments our full model obtains relative accuracy improvements of when compared to models that only treat language as descriptions of actionswe release our code and dataset at httpsgithubcomrewardsfromlanguage

a long line of work on grounded instruction following has developed various methods for producing actions from language including approaches that use intermediary structured semantic representations cite cite cite cite cite cite cite cite cite cite cite or map directly to primitive actions cite cite cite cite cite cite cite cite cite all of these approaches interpret any given utterance instruction solely in the context that elicited the utterance producing one particular sequence of actions the method we present extends these approaches using utterances to infer the rewards that underlie the actions that should be taken across a range of environments both the context that elicited the utterance and other unseen environments

the majority of work on reward learning has been in the robotics and reinforcement learning communities and has not incorporated language rather using techniques such as inverse reinforcement learning irl cite cite cite cite cite to infer the rewards that underlie human demonstrations of actions even works that incorporate language into reward learning also take this primarily actioncentric approach either by using datasets pairing utterances with trajectories and using languagefree irl to then recover reward functions from trajectories cite cite or learning an instructionfollowing model guided by a languageconditioned discriminator cite the language in these settings are unambiguous commands giving a complete description of a goal eg go to the red door in contrast we are concerned with language used to guide agents in repeated interactions where language may be a partial or ambiguous mix of instructions and reward descriptions figureeddd

a long line of work on pragmatics cite particularly in the rational speech acts rsa framework cite has developed computational models for inferring the behavior or belief that a speaker wishes to induce in a listener however the majority of this work has only focused on singleturn interactions where an utterance conveys an action in a single context eg choosing the correct referent in signaling games cite cite cite cite cite interpreting implicatures cite cite or generating cite cite or interpreting grounded instructions cite our work extends this past work by showing that in repeated interactions listeners can also benefit by reasoning pragmatically about how speakers communicate information about and over longer time horizons

we parameterize the users preference as a reward function formula with parameters formula in our flight booking domain from figure ref formula is a weight vector which specifies preferences over flight features carrier price etc we formalize the general reward inference problem as sequence of markov decision processes mdps formula that share the same reward function formula in each mdp formula the agent receives an utterance formula from the user and must execute a trajectory formula the agents goal is to infer formula over the sequence of interactions which should allow the agent to execute trajectories with high reward in asyet unseen contexts

the agent maintains an estimate over formula over the course of interactions we introduce a model formula that the agent will use to perform bayesian updates of a posterior over formula formula

in the flight domain we specialize this formulation to study a onestep mdp contextual bandit trajectories formula consist of a single action choosing one of the available flights over a series of these rounds where the agent books a flight given the users utterance formula the agent must infer the users flight preferences formula to book flights from other unseen sets of options without explicit language instruction from the user

our model summarized in figure ref defines a rational listener formula which predicts a distribution over rewards formula conditioned on an utterance formula and a context formula the terminology we use for listeners and speakers follows cite the rational listener uses bayesian reasoning about a speaker model formula which produces utterances conditioned on a reward function and context formula

key to our model is that the formula speaker distribution formula4faa defines how speakers produce language that functions both to elicit correct actions and describe their underlying reward formula

where formula controls the speakers nearsightednesshow much does the speaker care about the listener choosing the correct action in the current context rather than describing the reward in a contextindependent way so that the agent can make good choices in future contexts

the behavioroptimizing term formula specifies that the speaker chooses utterances that elicit rewardmaximizing behavior from a listener in the current environment formula

where the optimality model formula specifies the probability the speaker refers to trajectory formula if their true reward is formula we can formulate the optimality model with the boltzmann distribution common in irl where speakers are noisilyrational about which trajectories to refer to formula with rationality parameter formula this term specifies that utterances are more likely to refer to trajectories that have high reward according to the speakers formula compared to other trajectories in formula

then for a particular trajectory formula formula specifies what utterances are likely to refer to that trajectory in particular we model that speakers choose utterances that would make a listener execute that trajectory formula

using a base listener model formula of the type common in past work on instruction following we provide details on formula in section

finally we model formula the second term in formula with a base speaker model formula that maps rewards to reward descriptive utterances formulain principle formula could also do pragmatic reasoning to optimize a listeners reward belief but we did not find an improvement from doing so empirically we also provide details on formula in section

our account of pragmatic generation can also be viewed as the graphical model in figure ref c where importantly the reward influences the utterance both directly and via the action that the speaker refers to we define formula to be formula

and assume that utterances are rewarddescriptive in a way that is independent of the current context formula

we can confirm this leads us back to formula by marginalizing out formula formula

using this graphical model we illustrate how our model differs from prior work in similar settings

in general rsa allows the speaker to optimize for any utility function and in the simplest form the utility function optimizes for the listeners belief over world states cite however in most work on rsa the only relevant worldstate belief is belief about behavior eg the referent that should be selected figure ref instead our setting disentangles communication about intended referents in a single context and communication about reward beliefs which influence behavior on longer horizons cite cite have made the same observation reference games conflate whether the speakers objective is to influence beliefs or actions and modeling the speaker as one or the other produces distinct interpretations of utterances eg speakers that only optimize for correct behavior may do so at the cost of being truthful about the reward

prior work cite cite uses irl to recover rewards from trajectories eg from datasets pairing utterances with trajectories and then supervising a model with these induced utterance reward pairs while prior work has not specifically considered pragmatics ie speaker models their implicit speaker model amounts to assuming that all information about the reward comes from trajectories as in figure ref in our experiments we compare against a pragmatic version of this actioncentric speaker which is equivalent to setting formula in our model only using formulaacbd in realistic settings where utterances are not unambiguous commands like go to the red door it becomes important to model how actions and utterances reveal complementary information about rewards

we design flightpref a task for reward inference from natural language in the flight booking domain flightpref is designed to simulate a simplified interaction with a flight booking agent where users communicate with the agent via language to book flights from a set of options effective agents must not only learn to book the preferred flight given an instruction in the immediate context instruction following but also learn the users preferences over repeated interactions to book preferred flights in unseen contexts

we collect a dataset of natural language in a multiturn game between a user the speaker and an assistant the listener agent each flight is represented by a feature vector formula eg features of carrier price etc we assume the user has a linear reward function with parameters formula specifying a reward for a particular flight formula

in the first round of the game the user and assistant observe a set of three flight options and the user provides an utterance to describe the flight they want the optimal flight under the reward function eg the flight with the most stops in each of the subsequent rounds the user and assistant are presented with a new set of three flights the assistant can either choose by guessing the users preferred flight under the same reward function or prompt the user for another utterance describing the desired flight in the new set if the assistant chooses but does so incorrectly the user is prompted for another utterance describing the correct flight both players are penalized if the assistant chooses incorrectly and earn points if the assistant chooses correctly with more points for each round the assistant can do so without asking for help the user is thus incentivized to provide utterances that inform the agent which flight to choose while enabling longterm success over later rounds

to collect data for the task we recruit amazon mechanical turk workers and randomly pair them to play six games ie six different reward functions of six rounds each each game thus consists of 16 utterances describing options for the same reward function in different contexts one person plays the role of the user and the other acts as the assistant the user has access to a hidden reward function which is a discretized randomlysampled vector formula in total we collected 2 utterances across games of which we split off the games with the highest score where the speaker and listener were able to communicate most effectively for the evaluation set more details about the data collection process can be found in section of the appendix

a sampling of text is shown in figure ref utterances exhibit a range of phenomena some users lean towards describing very optionspecific features eg i like the flight that is other users attempt to describe as much of their reward function as possible eg i need a flight with any airline but jetbluewe note that even when they did so the users tradeoffs between features remain ambiguous many of the utterances are neither fully optionspecific nor fully rewarddescriptive instructions like one stop that is short both instruct the agent which flight to select in the present context while communicating some generalizable but incomplete information about the users preferences

our pragmatic model section ref relies on base listener and speaker models formula and formula4fbd in this section we describe implementations of these models for the flightpref dataset to train the base models we use the speakerside data of utterance option set reward function tuples from each round our base listener and speaker models assume that the utterances are generated conditionally independently given the reward we capture the dynamics of multiple turns in the posterior reward inference both base models learn neural encodings of utterances formula actions formula and rewards formula and produce distributions by applying softmax functions to inner products between these encodings we use formula to denote the optimal action in each context ie formula

the base listener model formula is defined using inner product similarities between learned representations of actions formula produced by an mlp encoder and learned representations of utterances produced by a bertbase cite encoder formula

where the distribution is normalized over all actions flights available in the context formulabadf

we set the rationality parameter formula4fec in formula8dfc as speakers tend to refer primarily to the optimal option in our domain

the base reward speaker model formula is defined using an inner product between representations of rewards formula from an mlp encoder and utterance representations from a bert encoder formula

where formula is normalized over a set of utterances taken from the training data see section in the appendix and formula is a temperature parameter

we finetune all model parameters including the parameters of the initiallypretrained bert utterance encoders in the listener and speaker on formula pairs from the training data using the adamw optimizer cite cite the listener and speaker models are trained separately without sharing any parameters between the encoders used in the two models we independently train 5 random seeds of each base model and ensemble them together in evaluation by averaging their output probabilities which we found improved performance of all models both our full model and baselines see section in the appendix for details and model hyperparameters

we follow previous work cite cite and approximate the formula9fcd distribution by normalizing over a fixed set of utterances the deduplicated set of short utterances less than 8 tokens making up the majority of utterances with no digits from the training data we implement the full pragmatic model formula in pyro cite and use importance sampling to generate samples from the posterior over rewards given our dataset collection procedure where we uniformly sample rewards we model an uniform prior over rewards formula for the first interaction

we evaluate models in the same repeated turn setup that humans carried out in the task for each game models play the role of the listener in that game updating the reward posterior section ref after observing the utterance and option set in each round our goal is to estimate rewards that allow the agent to carry out the persons preferences choosing the optimal option flight in unseen contexts sets of flight options to that end we directly compare models on heldout accuracy on 1 randomlygenerated sets of three options how often the models estimate of the reward formula selects the option that is optimal under the true rewardnote that when collecting the dataset we also tested human listenerss ability to generalize but only had them select an option on a single unseen option setthe next one in the sequenceto make data collection tractable we use the models reward posterior mean as the estimate formula we additionally provide comparisons of reward distance between the estimated reward and the true reward as a contextindependent metric formula where formula is the true reward

for our full action reward model we set the nearsightedness parameter formula for all posterior updates we compare to an actiononly model that uses only formula ie setting formula2dad this model is representative of approaches from past work on languageconditioned reward learning eg cite cite that infer rewards purely from the actions that utterances refer to we also compare to a rewardonly model that uses only formula inferring rewards purely from the utterance without conditioning on actions ie setting formula4dca for comparison to versions of our approach that remove pragmatic modeling see section ref in the appendix

in table ref we compare all models on heldout accuracy averaged over all rounds in the evaluation set for each round having observed all previous rounds in that game note that because heldout accuracy is assessed by the proportion of randomlygenerated flight sets out of 1 where the true reward function and the inferred reward function pick out the same optimal flight it is significantly more difficult than achieving high accuracy on a single threechoice instance

our full actionreward model achieves a heldout accuracy of 1 63 over the actiononly model and 13 over the rewardonly model indicating that combining both sources of information allows better inference of rewards that enable optimal actions in novel contexts for reference an oracle baseline that infers the value of formula randomly chosen features perfectly and is uniform on the other features obtains the following heldout accuracies formula1 2 3 4 showing that our model is able to attain similar generalization performance even in the presence of uncertainty without receiving oracle information about the true value of any feature

we analyze why our model benefits from both components in section ref and discuss potential for further improvements in section ref figure

we explore how each models reward inferences change as more observations are obtained over the course of a game in figure ref we plot heldout accuracy and distance to the true reward as a function of number of observed utterances our model outperforms the actiononly and rewardonly models for all numbers of observed utterances

while our full actionreward model improves substantially over the actiononly model at all points this improvement generally decreases as more utterances are observed figure ref conversely the improvement of the full model over rewardonly generally increases qualitatively we observe that this occurs because utterances tend to mention the most extreme features of the reward function which allow our model to estimate the values of these important features when there are few observations inferring reward information from utterances in this way is more informative than using only the option implied by the users utterance which does not disambiguate between rewards that select the same option a commonly discussed problem in irl cite

we observe that the actiononly model improves more consistently over rounds qualitatively the information that utterances provides about rewards is correlated across multiple roundsspeakers frequently mention salient reward features whereas actions consistently provide new information about all features this is particularly pronounced in our domain due to a relatively small feature and action space in other more complex domains actions might provide even more benefits as they provide finegrained information about reward values and tradeoff boundaries that are more difficult to communicate precisely in language

in this section we investigate why our model benefits from both the action and reward models

in figure ref we show the reward posteriors for each model after a single update on a round starting from a uniform prior in figure ref we observe how the action and rewardonly models can make correlated updates on an utterance and context where both the action a flight with a high value on arrival time and the utterance provide evidence about the arrival time feature this leads our models posteriors to aggregate more probability mass on positive values of that feature in figure ref we show how each model can make inferences about different features for the same contextthe actiononly model inferring positive values for arrival time given the observed flight and the rewardonly model updating on flight price and stops our model posterior aggregates information from both

another reason our full model improves is because some utterances are particularly farsightedmentioning a great deal of explicit information about the reward which the actiononly model cannot take advantage ofwhile other utterances are more nearsightedspecialized to the particular action eg saying just enough to uniquely identify the optimal flight sorting the utterances by difference in accuracy between the actiononly and rewardonly models confirms that they exhibit qualitatively different phenomena examples where the rewardonly model helps the most are highly rewarddescriptive eg if i had a choice i would never fly with delta and american get me jetblue or southwest while examples where the actiononly model helps most have less informative utterances eg the cheaper the better our full model is able to handle both kinds of language use

to further analyze the influence of the action and reward component we evaluate an oracle model that switches between the actiononly and rewardonly models choosing the model with highest heldout accuracy in each round this model outperforms our actionreward model improving from 1 to 9 on overall heldout accuracy suggesting that further improvements could be obtained by integrating evidence from the two models doing so optimally is challenging in our setting when a user says i like the cheap jetblue flight do they mean to say they like jetblue generally or just that they want to choose a desirable flight that happens to be uniquely identified by jetblue future work might explore adaptively switching policies eg using the utterance or knowledge about the user

while our base models have fairly high performance eg the base listener model formulaecad has an average accuracy of at selecting the optimal choice in each option set that has an utterance in the evaluation data they naturally have some errors which lead to errors in reward inference we test the influence of this underlying prediction error by skipping posterior updates on all rounds where the base listener predicts the incorrect option for the true reward function this change improves heldout accuracy by 6 over the rewardonly model after six observations 4 from the original gap indicating 1 that dataset affords future work on improved instruction following models and 2 that our reward inference procedure benefits from base model improvements

we note that in our task design the user does not provide a demonstration ie a choice of flight to the model however if it is convenient to obtain demonstrations from users eg a flight booking interface could let the person click on the flight they want in addition to specifying what they want in natural language demonstrations would effectively serve as an oracle instructionfollowing model for that context which could be incorporated into our full reward inference model

we presented a method for using natural language to infer reward functions representing the goals preferences and intents underlying action

conceptually our work builds on previous work on language grounding by exploring how language serves a dual purpose utterances can refer directly to actions to be taken as studied in instruction following beyond that they communicate information about why those actions should be taken and what actions may be desirable in new contexts to build languageguided agents that can interact with people over longer horizons it may be useful to model this relationship between language actions and rewards

furthermore language is ambiguous about both actions and goals standard settings for studying pragmatics eg reference games address how to resolve ambiguity about what object or action the speaker is choosing to refer to we have explored how these settings can be extended by considering the preferences underlying those choices we introduced flightpref a new dataset of naturalistic interactions between people in a multiturn flight booking game flightpref uses heldout accuracy as a metric for evaluating interpretation success beyond selecting the right action in a single environment

future work can build on the task by 1 learning or evaluating with more complex reward functions eg using deep reward representations 2 exploring how people communicate about their real preferences and modeling a natural prior eg that people tend to prefer cheaper flights instead of providing annotators with groundtruth preferences 3 allowing other ways to handle uncertainty eg leveraging the reward posterior to interactively learn to ask or 4 extending these approaches to other domains where modeling goals and preferences may be important eg languageconditioned robotics

we thank eric wallace jerry he and the other members of the berkeley nlp group and interact lab for helpful feedback and discussion this work is supported by a grant from the office of naval research onryip data collection we recruited amazon mechanical turk workers from the us with an approval rate of formula formula 5 human intelligence tasks hits completed and completion of a custom qualification task where they played minutes of the game and demonstrated active participation from manual review turk workers were given the following instructions for the task shortened scenario a customer working with a new personal assistant to book flights for their business meetings you will play either the customer or the assistant and another turker will play the other role as the customer you have specific preferences for what kind of flights you like and dislike when you first start working with your assistant you might need to tell them exactly what you want but you hope that over time they will figure out what you like and book your preferred flights without your help imagine an assistant that knows you well enough to say bob hates redeyes and doesnt like to be rushed on the way to the airport so ill go ahead and book this 2pm jetblue to new york as the assistant you want to figure out what the customer likes and book flights that they want pay attention to what they say when they choose flights what is this task for the goal of this task is to study how people naturally communicate their preferences to personal assistants with the goal of building digital assistants like siri that better understand and learn what people want for the purposes of the task we will give the customer fake flight preferences if you are the customer pretend that these are the kinds of flights you actually like dislike the interface for the task for an assistant is shown below figureworkers were compensated 4 for minutes 6 games of 6 rounds each with a 0 bonus for every points for each round points were accumulated based on the assistants action assistant chooses correctly points assistant chooses incorrectly points assistant asks for help points dataset details we plot the distribution of utterance lengths number of tokens in our dataset below figure base model details training we finetune all model parameters including the parameters of the initiallypretrained bert utterance encoders in the listener model and speaker model we produce utterance representations from bert using a linear projection of berts cls embedding for the utterance models are trained separately on the training data using the loss functions below with an adamw learning rate of formula and a batch size of for the listener model and a learning rate of formula and a batch size of for the speaker model we perform early stopping using the loss on heldout validation data listener loss we define the following loss function for the base listener model formula on each training instance formula speaker loss it would be possible to learn the parameters of the base reward speaker formula directly on the training data using a loss function similar to the base listener model however since utterances are often also actiondescriptive utterances cannot typically be predicted accurately from rewards alone to account for this we also define a separate base action speaker model formula that produces utterances conditioned on the optimal action formulabcba in context formula formula where formula is normalized over the same set of utterances as formula the base action speaker model is used only in training it would be possible to also use this base action speaker in evaluation in place of the pragmatic action speaker formula section ref however we found that pragmatic reasoning about a conventional instruction following model as outlined in section ref performs better we train the two base speaker models jointly using a simple latent variable model which makes the simplifying assumption that every utterance is either action or rewarddescriptive to model this we use a discrete latent variable formula8bfa formula9adb the loss function for a single example is formula4cbc where formulaaede gives the probability of an utterance being rewarddescriptive or actiondescriptive we model this using formula where formula is a parameter which is updated in training intuitively the latent variable model learns soft clusters for actiondescriptive and rewarddescriptive utterances with rewarddescriptive utterances providing stronger supervision for the formula model speaker normalization in training we compute the normalizers for the formula model using all utterances within the minibatch as well as up to 4 syntheticallyconstructed hardnegative examples defined by replacing attribute mentions within the true utterance detected using exact word match with alternative distractor attributes eg replacing any occurrences of jetblue with one of southwest american or delta randomly sampled we found that constructing hardnegatives in this way allowed us to train the base speaker models effectively despite using a fairly small dataset and small training batch sizes in evaluation we compute normalizers for the formula model using a filtered set of all utterances from the training data that contain no more than 8 words and no digits we use a smaller set of normalizers in training time for efficiency reasons hyperparameters mlps use fullyconnected layers with relu nonlinearities and dropout applied to each hidden representation during training we show hyperparameters for the models in table ref the bert model is bertbase implemented in huggingfaces transformers library cite table analysis analyzing the effect of pragmatically modeling the speaker tablewhile we do not expect pragmatically modeling an actiononly speaker will help in our domain since the action space is small there is little ambiguity in what the referenced action is we explore the effect of pragmatically modeling a beliefonly speaker we compare the beliefonly model to two nonpragmatic alternatives that directly infer formula0ceb without explicitly calculating the speakers distribution over utterances formula 1 inference normalizing the logits of the formula model over rewards formula rather than utterances and 2 training a formula model trained to maximize formula instead of formula we show the results in table ref our model outperforms on heldout accuracy by 56 over nonpragmatic alternatives suggesting that modeling the speaker distribution is helpful for interpreting utterances more accurately examples ranked by actiononly and beliefonly accuracy difference figurefigure ref shows utterances with the largest difference between actiononly and beliefonly heldout accuracy after updating independently on that round compared to two random other utterances in the validation set for examples where they both do better than chance the beliefonly model excels at rewarddescriptive utterances whereas actiononly flight tends to outperform when there is less information in the utterance

we recruited amazon mechanical turk workers from the us with an approval rate of formula formula 5 human intelligence tasks hits completed and completion of a custom qualification task where they played minutes of the game and demonstrated active participation from manual review turk workers were given the following instructions for the task shortened

scenario a customer working with a new personal assistant to book flights for their business meetings you will play either the customer or the assistant and another turker will play the other role

as the customer you have specific preferences for what kind of flights you like and dislike when you first start working with your assistant you might need to tell them exactly what you want but you hope that over time they will figure out what you like and book your preferred flights without your help imagine an assistant that knows you well enough to say bob hates redeyes and doesnt like to be rushed on the way to the airport so ill go ahead and book this 2pm jetblue to new york

as the assistant you want to figure out what the customer likes and book flights that they want pay attention to what they say when they choose flights

what is this task for the goal of this task is to study how people naturally communicate their preferences to personal assistants with the goal of building digital assistants like siri that better understand and learn what people want for the purposes of the task we will give the customer fake flight preferences if you are the customer pretend that these are the kinds of flights you actually like dislike

the interface for the task for an assistant is shown below figure

workers were compensated 4 for minutes 6 games of 6 rounds each with a 0 bonus for every points for each round points were accumulated based on the assistants action

assistant chooses correctly points assistant chooses incorrectly points assistant asks for help points

we plot the distribution of utterance lengths number of tokens in our dataset below figure

we finetune all model parameters including the parameters of the initiallypretrained bert utterance encoders in the listener model and speaker model we produce utterance representations from bert using a linear projection of berts cls embedding for the utterance models are trained separately on the training data using the loss functions below with an adamw learning rate of formula and a batch size of for the listener model and a learning rate of formula and a batch size of for the speaker model we perform early stopping using the loss on heldout validation data

we define the following loss function for the base listener model formula on each training instance formula

it would be possible to learn the parameters of the base reward speaker formula directly on the training data using a loss function similar to the base listener model however since utterances are often also actiondescriptive utterances cannot typically be predicted accurately from rewards alone to account for this we also define a separate base action speaker model formula that produces utterances conditioned on the optimal action formulabcba in context formula formula

where formula is normalized over the same set of utterances as formula the base action speaker model is used only in training it would be possible to also use this base action speaker in evaluation in place of the pragmatic action speaker formula section ref however we found that pragmatic reasoning about a conventional instruction following model as outlined in section ref performs better

we train the two base speaker models jointly using a simple latent variable model which makes the simplifying assumption that every utterance is either action or rewarddescriptive to model this we use a discrete latent variable formula8bfa formula9adb

the loss function for a single example is formula4cbc

where formulaaede gives the probability of an utterance being rewarddescriptive or actiondescriptive we model this using formula where formula is a parameter which is updated in training intuitively the latent variable model learns soft clusters for actiondescriptive and rewarddescriptive utterances with rewarddescriptive utterances providing stronger supervision for the formula model

in training we compute the normalizers for the formula model using all utterances within the minibatch as well as up to 4 syntheticallyconstructed hardnegative examples defined by replacing attribute mentions within the true utterance detected using exact word match with alternative distractor attributes eg replacing any occurrences of jetblue with one of southwest american or delta randomly sampled we found that constructing hardnegatives in this way allowed us to train the base speaker models effectively despite using a fairly small dataset and small training batch sizes

in evaluation we compute normalizers for the formula model using a filtered set of all utterances from the training data that contain no more than 8 words and no digits we use a smaller set of normalizers in training time for efficiency reasons

mlps use fullyconnected layers with relu nonlinearities and dropout applied to each hidden representation during training we show hyperparameters for the models in table ref the bert model is bertbase implemented in huggingfaces transformers library cite table

while we do not expect pragmatically modeling an actiononly speaker will help in our domain since the action space is small there is little ambiguity in what the referenced action is we explore the effect of pragmatically modeling a beliefonly speaker we compare the beliefonly model to two nonpragmatic alternatives that directly infer formula0ceb without explicitly calculating the speakers distribution over utterances formula 1 inference normalizing the logits of the formula model over rewards formula rather than utterances and 2 training a formula model trained to maximize formula instead of formula we show the results in table ref our model outperforms on heldout accuracy by 56 over nonpragmatic alternatives suggesting that modeling the speaker distribution is helpful for interpreting utterances more accurately

figure ref shows utterances with the largest difference between actiononly and beliefonly heldout accuracy after updating independently on that round compared to two random other utterances in the validation set for examples where they both do better than chance the beliefonly model excels at rewarddescriptive utterances whereas actiononly flight tends to outperform when there is less information in the utterance


