Semantic Relatedness for Keyword Disambiguation: Exploiting Different Embeddings  María G. Buey  magrabue@doctor.upv.es everis / NTT Data Universitat Politècnica de València València, Spain  Carlos Bobed  cbobed@unizar.es everis / NTT Data IIS Department, University of Zaragoza Zaragoza, Spain  Jorge Gracia  jogracia@unizar.es Aragon Institute of Engineering Research (I3A) / IIS Department, University of Zaragoza Zaragoza, Spain  Eduardo Mena  emena@unizar.es Aragon Institute of Engineering Research (I3A) / IIS Department, University of Zaragoza Zaragoza, Spain  ABSTRACT  Understanding the meaning of words is crucial for many tasks that involve human-machine interaction. This has been tackled by re- search in Word Sense Disambiguation (WSD) in the Natural Lan- guage Processing (NLP) field. Recently, WSD and many other NLP tasks have taken advantage of embeddings-based representation of words, sentences, and documents. However, when it comes to WSD, most embeddings models suffer from ambiguity as they do not capture the different possible meanings of the words. Even when they do, the list of possible meanings for a word (sense inventory) has to be known in advance at training time to be included in the embeddings space. Unfortunately, there are situations in which such a sense inventory is not known in advance (e.g., an ontology selected at run-time), or it evolves with time and its status diverges from the one at training time. This hampers the use of embeddings models for WSD. Furthermore, traditional WSD techniques do not perform well in situations in which the available linguistic information is very scarce, such as the case of keyword-based queries. In this paper, we propose an approach to keyword disambiguation which grounds on a semantic relatedness between words and senses provided by an external inventory (ontology) that is not known at training time. Building on previous works, we present a seman- tic relatedness measure that uses word embeddings, and explore different disambiguation algorithms to also exploit both word and sentence representations. Experimental results show that this ap- proach achieves results comparable with the state of the art when applied for WSD, without training for a particular domain.  CCS CONCEPTS  •   Computing methodologies   →   Natural language processing ;  Lexical semantics ;   •   Information systems   →   Similarity measures ;  KEYWORDS  Keyword Search, Semantic Relatedness, Word Embeddings, Word Sense Disambiguation  1   INTRODUCTION  In any information system which requires user interaction, being able to understand the user is a crucial requirement, which is often tackled by limiting the user input (e.g., presenting predefined forms with fixed options). The more freedom you provide the user with, the more difficult interpretation the computer has to do to achieve a useful interaction. In such a context, being capable of disambiguating the input words (i.e., associating each word with its proper meaning in a given context) is the starting point of any interpretation process done by the computer. Usually, such a disambiguation process is tackled from a Natural Language Processing (NLP) perspective [ 27 ], assuming rich linguis- tic information, such as Part of Speech (POS), dependencies between words, etc., which is very useful to perform the task. However, due to the world wide use of Web search engines, users are very used to keyword interfaces and they still express their needs in such terms. In this scenario, although there are some studies which point out that  keyword search queries   (aka.,   Web search queries ) exhibit their own language structure [ 3   ,   30   ,   32 ], we still need methods to disambiguate the meanings of the words which do not need such an information as it might not be available. Recent advances in NLP have focused on the development of dif- ferent embedding models [   4   ,   9 ,   21   – 23   ,   28 ], which are a set of language modeling and feature learning techniques where elements from a vocabulary are mapped to a vectorial space capturing their   distri- butional semantics   [   34 ]. While there are different methods to build word embeddings, the latest (and most successful) word embedding techniques rely on neural network architectures [ 4   ,   21 ,   23   ,   28   ]. Their usage as the underlying input representation has boosted the perfor- mance of different NLP tasks [ 35   ,   36   ]. However, in the context of dis- ambiguation tasks, one of the main limitations of word embeddings is that the possible meanings of a word are combined into a single repre- sentation, i.e., a single vector in the semantic space. Such a limitation can be avoided by representing individual meanings of words as distinct vectors in the space (e.g., sense2vec [   22   ]). However, there are scenarios where we do not know all the different senses at training time (e.g., open domain scenarios where we cannot find all the possi- ble meanings in a sense catalog), and, even if we would know them, we would require to have annotated data (which might be unavail- able or expensive to obtain). Besides, we would need to train a model for each new scenario or new meaning that would be added to our catalog. Thus, we need a disambiguation method able to relate words  arXiv:2002.11023v1 [cs.CL] 25 Feb 2020 and their senses in a flexible and general way (i.e., independently of the domain we are working in) by exploiting the available resources. In this paper, we propose a keyword disambiguation method which is based on the semantic relatedness (the degree in which two objects are related by any kind of semantic relationship [ 7   ]) between words, taking advantage of the semantic information captured by word embeddings. Our proposal makes possible to measure the relat- edness not only among plain words but also among senses of words (which, in a Semantic Web context, can be expressed as ontological terms), and it is able to work independently of the resources used, i.e., the sense inventory whose meanings we want to map to and the word embedding model used as input. For this purpose, we build on the work by Gracia and Mena on semantic relatedness [ 14 ] and disambiguation [ 15   ]. These works exploited the information about word co-occurrence frequencies provided by existing Web search engines. We evolve and adapt them to improve their performance using different kinds of embeddings (both at word and sentence level). The main benefit of such an adapta- tion is two-fold: 1) we exploit the semantics captured by embeddings which goes further than co-occurrence of terms, and 2) we decouple the proposal from any Web search engine, being able to use off- the-shelf models trained by third parties for our purposes. This has an important side-effect: our measure can be easily adapted to any domain which we have a document corpus from. Indeed, this adap- tation would require a training step, but it would be unsupervised and the only data required would be the corpus of documents itself. To evaluate our approach, we have carried out a thorough exper- imentation in the context of Word Sense Disambiguation (WSD), where we have used different pre-trained word embeddings pub- licly available on the Web, and WordNet 1   as sense repository. Our measure improves the performance obtained in [ 14 ], and achieves state of the art WSD values without the need of specific training for a specific sense inventory. This is especially relevant, for example, for systems based on keyword input and/or which have to work with dynamically selected ontologies [   5 ] or even with ontologies extracted directly from the Web [ 26   ]. All the experimental data and evaluation results are available online 2 . The rest of the paper is structured as follows. Section 2 discusses related works. In Section 3 we describe our semantic relatedness measure approximation, in Section 4 we present the disambiguation algorithm that we use, and Section 5 summarizes our experimental results. Finally, our conclusions and future work appear in Section 6.  2   RELATED WORK  Semantic relatedness is the degree in which two objects are related by any kind of semantic relationship [   7   ] and lies at the core of many applications in NLP (such as WSD, Information Retrieval, Natural Language Understanding, or Entity Recognition). The term is often confused with semantic similarity, which measures the degree in which two objects are similar or equivalent. For example, "car" is sim- ilar to "bus", but is also related to "road" and "driving". It has received a great research interest and different types of methods have been developed: it can be statistically estimated (e.g. co-occurrence-based  1 https://wordnet.princeton.edu/  2 https://bit.ly/2lqCzop  methods [ 18   ]) and learned (e.g., distributional measures that esti- mate semantic relatedness between terms using a multidimensional space model to correlate words and textual contexts [   25 ]); or it can be computed using a taxonomy or a graph (e.g., ontologies) to define the distance between terms or concepts [ 31   ]. Indeed, most methods rely on particular lexical resources (dictionaries, thesauri, or well structured taxonomies such as WordNet 1 ). Regarding disambiguation, WSD methods can be classified into four conventional approaches: supervised [   37   ], unsupervised [ 12   ], semi-supervised [ 38   ], and knowledge-based methods [ 10 ]. For ex- ample, in a way similar to us, in the SemEval 2015 All-Words Sense Disambiguation and Entity Linking task 3 , the majority of the ap- proaches (LIMSI, SUDOKU, EBL-Hope, etc.) that best performed in WSD relied on the combination of unsupervised learning of semantic information from the content of a corpus (such as SemCor) and/or on lexical resources as sense inventories (such as WordNet or BabelNet) to disambiguate the sense of words in natural language sentences. However, to our knowledge, no previous works (excepting those of Gracia and Mena [ 14   ,   15 ]) have studied specific methods for the dis- ambiguation of words in keyword-based inputs, where the linguistic information is scarce. Regarding the resources we use in our approach, word embed- dings represent words in a low-dimensional continuous space and they are used to capture syntactic and semantic information from massive amounts of textual content. In recent years, they have gained great popularity due to this ability and many NLP appli- cations have taken advantage of the potential of these distributional models. Bengio et al. [ 4 ] preceded a wide number of current lan- guage model techniques and several authors have proposed their own approaches [ 9   ,   21   ,   28   ] to construct word embeddings vectors where   word2vec   [23] is the most widely used. Despite their advantages, one of the main limitations of word embeddings is that possible meanings of a word are conflated into a single representation. Sense embeddings (e.g., sense2vec [   22   ]) are proposed as a solution to this problem: individual meanings of words are represented as distinct vectors in the space. These approaches are classified in two categories by how they model meaning and where they obtain it from [   8 ]: 1) unsupervised models which learn word senses from text corpora (by inducing different senses of a word, ana- lyzing its contextual semantics in a text corpus and representing each sense based on the statistical knowledge derived from the corpus), and 2) knowledge-based methods which exploit sense inventories of lexical resources for representing meanings (such as WordNet 1 , Wikipedia 4 , BabelNet 5 , etc.). We can also find models that not only provide representations of words, but also the senses of the words in a joint embedded space. This is the case of NASARI vectors [ 9 ] which not only provide accurate representation of word senses in different languages, but they also include both concepts and named entities, all included in a single unified semantic space. However, in the first case (i.e., unsupervised models), we cannot target a par- ticular sense inventory or ontology to perform the disambiguation, not having control for example about the concept detail/granularity, and, besides, the detected senses might not be aligned to any par- ticular human-readable structure; in the second case, we need to  3 http://alt.qcri.org/semeval2015/task13/  4 https://www.wikipedia.org/  5 https://babelnet.org/  2 know all the senses at training time, not being able to adapt to new scenarios (e.g., addition/deletion of senses in the directory, evolving ontologies, etc.). Thus, the sweet spot would be neither to require re-training nor newly labelled data, while being capable to perform the disambiguation against any sense repository. Although sense embeddings capture and represent information about meanings and can be used to calculate the sense that a word has in a specific context, word embeddings have also been shown to have good performance in disambiguation tasks [   16 ]. Therefore, we wanted to explore how we could push further the usage of word embeddings for keyword disambiguation. Working at word level (as starting point) allows us to use a semantic relatedness measure between terms and reuse available resources, without needing to train explicitly new word embeddings neither for a specific task nor for newly added possible senses (i.e., adapting to any given sense dic- tionary or ontology). We have taken as baseline the works presented in [ 14   ,   15   ]. In [ 15   ], the authors provide a keyword disambiguation algorithm that uses the semantic relatedness measure defined in [ 14   ] to find the appropriate sense for keywords. The authors focused on a method that exploits the Web as a source of knowledge, and a transfor- mation of the Normalized Google Distance (NGD) [   11   ] into a mixed way of relatedness measure (between ontology terms and plain words). We propose to, on the one hand, substitute this distance with a word embedding based one to take advantage of the semantics cap- tured by embeddings, improving the performance regarding using just co-occurrence of terms; and, on the other hand, explore modifica- tions of their algorithm to improve their disambiguation capabilities. Finally, as pointed out by Lastra-Díaz et al. [   19   ], the embeddings that behave the best for disambiguation purposes are those which capture not only distributional semantics of texts, but also structural information about the possible meanings. We aim at achieving this disambiguation performance in a more flexible way, decoupling lin- guistic surface from the actual sense catalog (i.e., ontology) in order to adapt to new (i.e., unknown at training time) possible meanings, and being capable to apply it to keyword inputs, where the linguistic information is scarce.  3   RELATEDNESS MEASURE BASED ON WORD EMBEDDINGS  Word embeddings can be used out-of-the-box to compute relatedness between words. However, they do not suffice in situations in which a relatedness has to be computed between senses (e.g., ontology terms in a Semantic Web context) or between senses and words. To that end, we ground on a previously defined relatedness measure between senses proposed by Gracia and Mena [ 14   ]. The authors proposed a method to compute the semantic relatedness between ontology terms (which we can see as individual senses), and an extension to calculate it between plain words and terms. Their proposal was built on the notion of the   ontological context   of a term, which is constructed combining the synonyms and the hypernyms of the ontological term (or sense). Given an ontological term   t , they defined its   ontological context   (denoted by   OC ( t ) ) as the minimum set of other ontological terms that belong to its semantic description, locating the term in the ontology and characterizing its meaning. For example, in the WordNet taxonomy, the class “Java” (in the sense of “an Indonesian  Figure 1: Example of the semantic description of the term "Java" in WordNet.  island”), is well characterized and distinguished from other senses by considering its direct hypernym “Island” (see Figure 1). Then, given two ontological terms   a   and   b , their relatedness mea- sure is computed as:  rel ( a , b )   = w 0 rel 0 ( a , b ) + w 1 rel 1 ( a , b ) ,  ( w 0   ≥   0 , w 1   ≥   0 , w 0   + w 1   =   1 )   (1) with   rel 0 ( a , b )   and   rel 1 ( a , b )   computed as follows:  rel 0 ( a , b )   =  Í i ,   j   rel w   ( s y n a i   , s y n b j   ) | S y n ( a )|| S y n ( b )|   ,  ( i   =   1 .. | S y n ( a )| , j   =   1 .. | S y n ( b )|)  (2)  rel 1 ( a , b )   =  Í i ,   j   rel 0 ( oc a i   , oc b j   ) | OC ( a )|| OC ( b )|   ,  ( i   =   1 .. | OC ( a )| , j   =   1 .. | OC ( b )|)  (3) where   rel w   refers to the relatedness between words (as it will be defined later on in equations 7 and 9);   S y n ( a )   =   { s y n a   1 , s y n a   2 ,... }  and   S y n ( b )   =   { s y n b   1 , s y n b   2 ,... }   are the set of synonyms (equivalent labels, including the term label) of ontological terms   a   and   b ;   OC ( a )   =  { oc a   1 , oc a   2 ,... }   and   OC ( b )   =   { oc b   1 , oc b   2 ,... }   are the terms of their on- tological context 6 . Each   a   and   b   is characterized by taking into ac- count two levels of their semantic description:   Level 0)   the term label and its synonyms (Equation 2), and   Level 1)   its ontological context (Equation 3).   w 0   and   w 1   are used to weight these levels 7 . This measure can be also applied between an ontology term   t   and a plain word   w   which provides us with a value which indicates the relatedness degree between a sense and a word. So, in that case, the previous equations are computed as follows:  rel ( t , w )   = w 0 rel 0 ( t , w ) + w 1 rel 1 ( t , w ) ,  ( w 0   ≥   0 , w 1   ≥   0 , w 0   + w 1   =   1 )   (4)  rel 0 ( t , w )   =  Í i ,   j   rel w   ( s y n t i   , w ) | S y n ( t )|   , ( i   =   1 .. | S y n ( t )|)   (5)  rel 1 ( t , w )   =  Í i ,   j   rel 0 ( oc t i   , w ) | OC ( t )|   , ( i   =   1 .. | OC ( t )|)   (6)  6 Notice that   | S y n ( x   )| ≥   1   and   | OC ( x   )| ≥   0 .  7 We set these values as   w 0   = w 1   =   0 . 5   as indicated in [14].  3 Here,   rel w   is the distance that the authors used in [ 14   ] to mea- sure how two plain words are related. They proposed a general- ization of the Cilibrasi and Vitányi’s Normalized Google Distance  NGD(x,y)   [ 11   ] to use any Web search engine as source of frequencies. This generalization is called Normalized Web Distance   NWD(x,y) , whose smaller values represent greater semantic relation between words. Although most of NWD values fall between 0 and 1, it ranges from 0 to   ∞ . Therefore, to obtain a proper relatedness measure in the range [0, 1] that increases inversely to distance, they proposed the following transformation:  rel w   ( x , y )   = relW eb ( x , y )   = e − 2 N W D ( x , y )   (7) To explore the use of emerging word-embedding techniques in this context and compare them with those based on search engines, we propose to exploit the semantic capabilities of word embeddings in this formulation and substitute the   relW eb ( x , y )   measure. We could use the cosine similarity distance between the embedding vectors of the words, i.e., using the following equation:  sim ( x , y )   = cos ( θ   )   =   X   1 • X   2  || X   1 || · || X   2 ||   (8) where   x   and   y   are plain words,   X   1   and   X   2   their correspondent word embedding vectors, and   θ   the angle between them. However,  sim ( x , y )   ranges in [-1, 1], so, in order to obtain a distance in the range [0, 1] (so that we can substitute Equation 7 directly in Equation 2), we propose to use the   angular distance   instead, which is computed as follows:  rel w   ( x , y )   = an д . distance ( x , y )   =   1 −   arccos ( sim ( x , y ))  π   (9) So, in Equation 2, we use Equation 9 as   rel w   distance instead of Equa- tion 7. We use this distance to compute the semantic relatedness between words, between ontology terms (or senses), or between ontology terms and words, obtaining a value between 0 and 1. For those cases in which the label of the ontological term is multi-word, we just compute the centroid of the set of words that form the label. While, at first, it might seem that we limit the coverage of the mea- sure proposed in [   14 ] (it built on the results of Web search engines, which potentially cover any domain), we have to bear in mind the plethora of word embedding models directly available in the Web, as well as the possibility of using our own corpus of documents to fine tuning our measure for a particular domain (which is easier to have, rather than crawling the whole WWW).  4   DISAMBIGUATION ALGORITHM  We ground our keyword disambiguation proposal on the disam- biguation algorithm defined in [   15   ], using the adapted semantic relatedness measure proposed in the previous section. This algo- rithm is based on the hypothesis that the most significant words in the disambiguation context are the most highly related to the word to disambiguate; such words conform the   active context   of the word being disambiguated. As an overview, once the   active context   of each input keyword has been calculated, the algorithm performs three main steps: 1) obtain- ing the semantic relatedness between the active context of a keyword and its possible senses, 2) calculating the overlap between the words in the active context and the semantic descriptions (i.e.,   ontological context ) of the possible senses of the keyword to disambiguate, and 3) re-ranking the possible senses according to their frequency of use (only when such information is available for the sense inventory se- lected 8 ). Apart from using the updated   rel w   measure to select the   ac- tive contexts , we propose to modify the second step of this algorithm in order to study the influence of different approaches which exploit the semantic information captured by different word embeddings. In the following subsections, we first detail the original algorithm which we base our proposal on, and, then, we describe the modifications that we propose to improve its performance using word embeddings.  4.1   Background: Algorithm Description  First of all, let us formally introduce the notion of   active context . Let   k  be an element of an input sequence of words   S   with an intended meaning,   K   ⊆   S   be the set of all possible keywords in the input,   C   ⊆   K  the set of keywords of the disambiguation context (i.e., the complete disambiguation window considered, e.g., the sentence where the key- word appear), and   k d   ∈   K   the target keyword to disambiguate. Thus:  Definition 4.1.   Given a context   C   ∈   K , and a word to disambiguate  k d   ∈   K , the   active context   C a   of   k d   is a subset   C a   ⊆ C   such that  ∀ k i   ∈ C a   ,  k j   ∈ ( C   − C a   ) ∋ rel ( k j   , k d   )   > rel ( k i   , k d   ) . In other words,   C a   contains the words in the input that are the most related ones to   k d   . To obtain such a context, we stick to the method proposed in [   15 ]: 1) removing repeated words and stop- words from   C , 2) applying a semantic relatedness ( rel w   in our case) between each context word   k i   ∈ C   and the keyword to disambiguate  k d   , and 3) constructing C a   with the context words whose relatedness scores above a certain threshold. The output of this process is the  active context   C a   ⊆ C . The maximum cardinality of   C a   is set to a fixed value( | C a   |   =   4 ) following Kaplan’s experiments [17]. Once we have obtained   C a   for   k d   , we can apply the main algo- rithm, which takes as input   k d   ,   C a   , and a set of possible senses for  k d   ,   S k d   . The main steps are presented in Algorithm 1 9 : (1)   Applying the semantic relatedness:   First, the algorithm computes an initial disambiguation between the senses in  S k d   and the active context   C a   (Lines 2-7). For this, we use the updated relatedness measure presented in the previous section (Equations 4 and 9). The score assigned to each sense ( score s i   ) is the mean of   rel ( s i   , k j   )   where   s i   ∈   S k d   is a candidate sense of the keyword being disambiguated, and   k j   ∈ C a   is a keyword in the active context. (2)   Calculating the context overlap:   The disambiguation al- gorithm weights the scores taking into account the overlap between   C a   and the ontological context of each sense,   OC ( s i   )  (Lines 8-11). Note that   OC ( s i   )   includes its synonyms, glosses, and labels, as well as labels of other related terms, such as hy- pernyms, hyponyms, meronyms, holonyms, etc. The overlap is calculated (ignoring stopwords) as:  o v erlap ( C a   , OC ( s i   ))   =   | OC ( s i   )∩ C a   |  min (| OC ( s i   )| , | C a   |)  (3)   Frequency of usage:   Finally, the frequency of use of the high- est scored senses is taken into account (Lines 12- 17), if such information is available. The proximity decision is handled by  8 If we do not have such information, we assume that all senses are equally likely.  9 We refer the interested reader to [15] for the complete details.  4 Algorithm 1:   Keyword disambiguation algorithm  Input   :  K d   : The keyword to disambiguate.  S k d   : The set of possible senses for   K d   .  C a   : The active context selected for   K d   .  Output : A weight for each sense   s i   ∈   S k d   .  1   function   disambiguate   ( K d   , S k d   , C a   ) :  2   foreach   sense   s i   ∈   S k d   do  3   foreach   keyword   k j   ∈ C a   do  4   r j   = rel ( s i   , k j   )  5   end  6   score s i   =   Í j   r j   /| C a   |  7   end  8   maxScore   = max ( score s 1   ,..., score s n   )  9   foreach   sense   s i   ∈   S k d   do  10   newScore   = score s i   + ( 1 − maxScore )∗ o v erlap ( C a   , OC ( s i   ))  score s i   = newScore  11   end  12   maxScore   = max ( score s 1   ,..., score s n   )  13   foreach   sense   s i   ∈   S k d   do  14   if   score s i   > proximit y Factor   ∗ maxScore   then  15   newScore   = score s i   + ( 1 − maxScore )∗ normFreq ( s i   )  score s i   = newScore  16   end  17   end  a   proximit y Factor   ∈ [ 0 , 1 ] , which is combined with the maxi- mum of the scores of the senses ( proximit y Factor   ∗ maxScore ) to obtain a threshold. The scores of the senses   s i   which are above such a threshold are then updated using:  normFreq ( s i   )   =  √  a ∗   f requenc y s i  Í j   f requenc y s j  + b  where   Í j   f requenc y s j   is equal to the sum of the frequency of all senses of   k d   ,   a   and   b   are constrained 10   by   a , b   ∈ [ 0 , 1 ]   and  a + b   =   1 . The output of the disambiguation algorithm is a score for each possible sense   s i   ∈   S k d   that represents the confidence level of being the right sense according to the active context   C a   . Note that, in our approach,   S k d   is not restricted to any particular dictionary, as it could be dynamically built from, e.g., different ontological resources.  4.2   Proposed Modifications  As our aim is to study the best way to exploit word embeddings, we have analyzed their characteristics and explored different ap- proaches to use them in the adopted disambiguation process. In particular, in this section, we present a list of possible modifications to the Step 2 of the algorithm (Lines 8-11) to include and take advan- tage of the properties of word embeddings along with the rationale behind them. For the rest of the section, let   maxScore   be the maxi- mum score among all senses in   S k d   ,   centroid   a function to calculate an average vector by the arithmetic mean of a set of vectors, and   rel w  10 We set   a   = b   =   0 . 5   and   pr ox imit y   F act or   =   0 . 75   as indicated in [15].  the angular distance in Equation 9. Thus, the different approaches are described below:  •   Average:   The straightforward way to include the embeddings is to calculate directly the average vector of the different bag of words involved in the disambiguation, under the assumption that the semantically coherent groups of words should out- stand from the others. Thus, instead of computing the overlap between the semantic descriptions   OC ( s i   )   of each sense   s i  and the current active context   C a   , we propose to compute the average between the word vectors from   C a   and   OC ( s i   )   to obtain a new score. Line 10 in Algorithm 1 changes to:  newScore ( s i   )   = score s i   + ( 1 − maxScore )∗ a v era д e ( C a   , OC ( s i   ))  where   a v era д e ( C a   , OC ( s i   ))   is:  a v era д e ( C a   , OC ( s i   ))   =  Í i ,   j   rel w   ( k i   , w j   ) | C a   || OC ( s i   )|   ,  ( k i   ∈ C a   , w j   ∈ OC ( s i   ) , i   =   1 .. | C a   | , j   =   1 .. | OC ( s i   )|)  That is, we consider each set of words as a cluster in the vec- tor space, and we represent them by their centroid. If there are elements that do not contribute to the semantic cohesion of the clusters, they will contribute negatively (they will in- crease the semantic distance) to select a particular sense for the target keyword 11 .  •   Sense centroid without most frequent component:   As an evolution of the previous method, we studied the method described by Arora et al. [   1 ], called   Smooth Inverse Frequency (SIF) . They propose to represent a sentence by a weighted average vector of its word vectors which the most frequent component using PCA/SVD is substracted from. Thus, we propose to consider the semantic description   OC ( s i   )   of all senses of the sense inventory as sentences, and to calculate the SIF embedding of them. Then, during the disambiguation, we compute a new score (Line 10) for the sense being consid- ered   s i   by measuring the distance between the centroid of the active context   C a   and the SIF vector of each   OC ( s i   ) , following this computation:  newScore ( s i   )   = score si   + ( 1 − maxScore )∗  rel w   ( centroid ( C a   ) , SI F   ( OC ( s i   )))  Note that we do not substract the SIF vector from   C a   as all its words are already deemed as important. The most frequent component vector we are removing may encompass those words that occur most frequently in a corpus and lack seman- tic content (e.g., stop-words), thus not contributing to the actual disambiguation.  •   Top-K nearest words:   As a variant of the two previous meth- ods, in this method, we select the top   k   nearest words from the semantic description   OC ( s i   )   of a sense to   C a   ∪ k d   . After that, we compute the distance between centroids of the active context and the top K nearest words selected to obtain its new score:  newScore ( s i   )   = score s i   + ( 1 − maxScore )∗  rel w   ( centroid ( C a   ) , centroid (  11 We also studied other cluster-based distances measures (e.g., single linkage), but the results did not improve using the centroid-based measure, so we focused on the average vector which is broadly used in the literature.  5 topKN earest ( centroid ( C a   ) , OC ( s i   ))))  In this case, we work under the same hypothesis as for the se- lecing an   active context : the words that belong to the semantic description of the sense that are the closest ones to the active context and the keyword that is being disambiguated, should be the most significant to contribute in making a correct dis- ambiguation.  •   Doc2vec:   Finally, instead of treating the ontological descrip- tions as bag of words in this method, we consider them as proper documents and apply   doc2vec   [ 21   ]. In particular, each semantic description   OC ( s i   )   of the senses becomes a docu- ment, and   doc2vec   allows to calculate an embedding space for all of them. Then, we compute the distance between the cen- troid of the active context   C a   and the embedding calculated for the sense. Note that   doc2vec   learns as well a word embed- dings model that it uses during training. We use those word vectors to create the centroid of the active context. Therefore, in a similar way, the new score is computed as:  newScore ( s i   )   = score s i   + ( 1 − maxScore )∗  rel w   ( centroid ( C a   ) , doc 2 v ec ( OC ( s i   )))  We consider the semantic descriptions as documents to cap- ture the distributional semantics both at local (window) and global (document) scope. We report the best results that we obtained by applying these different approaches in the following section.  5   EXPERIMENTAL EVALUATION  In this section, we discuss the results obtained in the experiments that we have carried out to evaluate our proposal. Firstly, we evaluated different available embedding models using the distance proposed in Equation 9. We performed several tests comparing to human judg- ment in order to check how the   angular distance   behaved. Secondly, we evaluated the potential of our keyword disambiguation algorithm and the relatedness measure among ontology terms and words in the context of WSD, including all algorithm variations that we have proposed in Section 4.2. For the experiments, we used the following pre-trained vectors:  word2vec   trained on Google News corpus 12 ,   word2vec   trained on Wikipedia 13 ,   doc2vec   trained also on Wikipedia 14 ,   GloVe   trained on Wikipedia 2014 and Gigaword 5 corpus 15 , and   N ASARI embed   along with the   word2vec   word embeddings trained on the UMBC corpus 16 . We used WordNet 1   as the sense inventory.  5.1   Correlation with Human Judgment  In order to validate the hypothesis of the suitability of using word embeddings along with the use of the   angular distance   to compute semantic relatedness, we first analysed the correlation of such a tech- nique with human judgment in a basic word-to-word comparison. For this purpose, we used different datasets available in the litera- ture which contain pairs of words whose relatedness was manually  12 https://code.google.com/archive/p/word2vec/  13 https://github.com/jhlau/doc2vec  14 Dump dated in 2015-12-01.  15 https://nlp.stanford.edu/projects/glove/  16 http://lcl.uniroma1.it/nasari/#two  assessed by different people. The datasets and their details can be seen in Table 1.  Table 1: Correlation with human judgment benchmarks.  Dataset   #Word Pairs   #Human Judges MC-30 [24]   30   38 WordS353-Rel [13]   252   13 WordS353-Sim [13]   203   16 RG-65 [33]   65   51 MEN dataset (train/dev) [6]   2000/1000   crowdsourced 17  GM dataset [14]   30   30 The results obtained for Spearman correlation are presented in Table 2. Reported values, where available, were calculated using the widespread used cosine similarity. We can see that using the angular distance (Equation 9) to calculate relatedness between pairs of words also offers a semantic correlation with the human judgment. In particular, regarding the GM dataset [ 14   ], the authors reported a 78% using the previous   relWeb   measure (Equation 7). We can see a strong improvement in this dataset by using word embeddings: we achieve up to a 87.3% using   word2vec   trained on Google News (taking into account the average of all models, we achieve an average of 81.2% for this dataset). These results enable us to use the angular distance as the core relatedness measure in Equations 1 to 3. Note that for   word2vec   and   doc2vec   trained on Wikipedia we can not pro- vide a comparison, because Lau & Baldwin [ 20 ] did not evaluate the correlation with human judgment.  5.2   Word Sense Disambiguation Evaluation  To evaluate our proposal, we used three datasets oriented to WSD: SemCor 2.0 dataset 18 , SemEval2013 all-words WSD dataset 19 , and SemEval2015 All-Words Sense Disambiguation and Entity Linking dataset 3 . We used WordNet 1   as sense inventory. For SemCor 2.0, we specifically used WordNet 2.0 as such a dataset is annotated with this version, and for the rest of datasets we used WordNet 3.0. We tested all the options proposed in Section 4.2 for the disam- biguation algorithm, and we obtained that the   Top-K nearest words  option achieves the best results 20 . Thus, due to space restrictions, we focus on   Top-K nearest words   option in this section 21 . Regarding the models, we selected   word2vec   trained in Google News and   word2vec  trained in Wikipedia because they showed better average correla- tion with human judgment in different datasets (see Table 2); and  N ASARI embed   + U MBCw 2 v   word embeddings because, although they do not excelled in correlation with human judgment, they showed the best performance in all test datasets for WSD. Finally, in order to compare the results to [ 14   ], we report the precision results for SemCor 2.0; while we report the F-score results for the rest of datasets.  SemCor2.0 Experiments:   Following [ 14   ], in this set of experiments, for each of three selected highly ambiguous nouns ( plant ,   glass , and  earth ), we took 10 random sentences from the corpus. Table 3 presents  17 They used Amazon Mechanical Turk: https://www.mturk.com/  18 http://web.eecs.umich.edu/~mihalcea/downloads.html#semcor  19 https://www.cs.york.ac.uk/semeval-2013/task12.html  20 Achieving the best performance for   K   =   15 .  21 The interested reader can find all the details of the experiments at https://bit.ly/2lqCzop  6 Table 2: Spearman correlation coefficients between the angular distance applied on word pairs and human judgment in different datasets. Upper values are our evaluations, lower ones are the reported values in the original papers using the cosine distance. Highlighted values equal or outperform the best result (78%) for the same dataset in [14].  Vectors\Datasets   MC-30   WS353-Sim   WS353-Rel   RG-65   MEN   GM   Average GloVe   70.4   66.5   56.1   76.9   74.2   84.5   71.4 Reported at Pennington et al. [28]   72.7   65.8   -   77.8   -   -   72.1 Google News word2vec   80.0   77.2   63.5   76.0   77.0   87.3   76.8 Reported at Camacho-Collados et al. [9]   80.0   77.0   -   -   -   -   78.5  N ASARI embed   + U MBCw 2 v   70.3   72.7   56.8   70.7   74.5   74.7   70.0 Reported at Camacho-Collados et al. [9]   83.0   68.0   -   80.0   -   -   75.5 Wikipedia word2vec   80.9   77.9   62.2   78.3   76.9   81.8   76.3 Reported at Lau & Baldwin [20]   -   -   -   -   -   -   - Wikipedia doc2vec   73.3   69.0   52.3   71.6   72.0   77.8   69.3 Reported at Lau & Baldwin [20]   -   -   -   -   -   -   - the results: all cases outperform the results achieved in [14], which reported an average precision of 57%. Our best performance is an av- erage precision of 63.15% with   N ASARI embed   + U MBCw 2 v   vectors. In fact, SIF method shows equal or even slightly better performance in this particular dataset using   N ASARI embed   + U MBCw 2 v   vectors and   word2vec   trained in Google News vectors. However, in the rest of cases it is   Top-K nearest words   method that obtains the best results. In addition, SIF method requires to preprocess the target sense in- ventory to calculate the sentence embeddings, introducing a mild dependence to it. Our selected method shows a good performance (it improves the results of the original algorithm), while allowing to be more decoupled from the actual sense inventory used.  SemEval Results:   In Table 4, we present the results obtained for Se- mEval 2013 and SemEval 2015. In this case,   N ASARI embed   + U MBC w 2 v   vectors achieved the best results, with an F-score of 64.39%. In SemEval 2013, UMCC-DLSI reported the best results, with a F-score of 64.7%, similar to ours. Besides, our results are similar to other state-of-the-art systems using sense embeddings: Camacho et al. [   9   ] reported an F-Score value of 66.7% with their   N ASARI l exical   vec- tors evaluated in SemEval 2013. Unfortunately, they do not provide results for their   N ASARI embed   vectors using WordNet. Regarding SemEval 2015, the best reported result in our task reached an F-score of 65.8%, while the baseline used to compare systems (BabelNet First Sense (BFS)) was an F-score of 67.5%. We reach an F-score value of 61.61%, which, while does not beat previous values, is meritory given that our approach is focused on situations where linguistic information might be scarce (e.g., keyword-based input). To sum up, our proposal improves the results presented in [ 14   ] by substituting their Web search engine-based measure to one that uses word embeddings. We also improve the disambiguation results reported in [   15   ] by adapting their algorithm to exploit the properties of the word embeddings. Our proposal achieves similar performance levels to the SOTA, while providing the flexibility to work indepen- dently of the resources used (i.e., word embeddings, sense inventory), and reducing the barriers to its application to any domain.  6   CONCLUSIONS AND FUTURE WORK  In this paper, we have presented a keyword disambiguation approach based on a semantic relatedness measure which exploits the seman- tic information captured by word embeddings and tries to map the meanings from a sense inventory. We have visited the semantic re- latedness measure proposed in [   14   ] to adapt it to work with word embeddings instead of relying on Web search engines, and we have improved a disambiguation algorithm [   15   ] by exploring different uses and types of embeddings (both at word and sentence level). To validate our proposal, we have performed several experiments around Word Sense Disambiguation (WSD) where we have used different pre-trained word embeddings and WordNet as the resource to obtain the target senses of words. With our proposal:  •   We are able to relate words and meanings from a sense in- ventory (e.g., ontology terms) in a flexible way, by exploiting available resources and regardless the domain in which we are working. This makes our measure adaptive and general enough to be used for different contexts.  •   We provide a method which can be adapted to any domain in a dictionary-decoupled way, provided that we have a document corpus which would allow us to capture the distributional semantics. This lowers the requirements of data in order to build more specific models for particular domains.  •   We have tested the capabilities of different word embedding models, improving the results presented in [   14 ]. We evalu- ated our measure in the same SemCor 2.0 dataset described in this work, and we have obtained in the best case an aver- age increase of 6% in precision (a relative improvement of about 11%).  •   Being decoupled from a fixed pool of senses does not come at the expense of performance. We achieve similar quality of re- sults than having an ad hoc and more expensive trained model capturing the possible senses. In particular, we have tested our measure in SemEval 2013 and SemEval 2015 datasets reaching an F-score of 64.39% and 61.61% respectively. These results are similar to the state of the art [9] using   sense2vec   approaches. As future work, we want to further extend our approach to the field of concept discovery (similar to entity search [ 2   ], but focused on concepts rather than on instances). We also want to explore newer  7 Table 3: Precision results for SemCor 2.0 dataset (10 random sentences) adopting Top-K nearest words. The two rightmost columns show the results using the   relWeb   based relatedness measure and Most Frequent Sense methods as reported in [14].  Experiment\Approach   Wikipedia word2vec   Google News word2vec   N ASARI embed   + U MBCw 2 v   relWeb *   Most Freq. Sense* 10 sent. with PLANT   58.44%   63.03%   66.20%   80%   40% 10 sent. with GLASS   57.47%   63.78%   60.15%   30%   30% 10 sent. with EARTH   59.21%   56.38%   62.33%   60%   60% AVERAGE   58.41%   61.13%   63.15%   57%   43%  Table 4: F-Score results for SemEval 2013 and 2015 datasets adopting Top-K nearest words. Values in column highlighted by   ⋄  correspond to the best system in each SemEval dataset. Values in column highlighted by   ±   are the baselines reported in SemEval 2013 and SemEval 2015.  Wikipedia word2vec   Google News word2vec   N ASARI embed   + U MBCw 2 v   Best system ⋄   Baseline ±  SemEval 2013   59.59%   62.81%   64.39%   66.7%   63.0% SemEval 2015   61.32%   60.37%   61.61%   65.8%   67.5% contextualized word embeddings, such as BERT or XLNet (based in ELMo [   29   ]), and how they could be used in this context. Finally, we would like to propose a specific dataset exclusively for keyword disambiguation taking QALD 22   datasets as baseline; we want to develop it in order to test our relatedness measure in a more appro- priate dataset for the context in which we focus: the disambiguation of keyword-based inputs.  REFERENCES  [1]   S. Arora, Y. Liang, and T. Ma. 2017. A Simple but Tough-to-Beat Baseline for Sen- tence Embeddings. In   Proc. of Intl. Conf. on Learning Representations (ICLR’17) . 1–16. [2]   K. Balog. 2018.   Encyclopedia of Database Systems .   Chapter Entity Retrieval, 1326–1331. [3]   C. Barr, R. Jones, and M. Regelson. 2008.   The Linguistic Structure of English Web-Search Queries. In   Proc. of Conf. on Empirical Methods in Natural Language Processing (EMNLP’08) . 1021–1030. [4]   Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A Neural Probabilistic Language Model.   Journal of Machine Learning Research   3, 6 (2003), 1137–1155. [5]   C. Bobed and E. Mena. 2016. QueryGen: Semantic interpretation of keyword queries over heterogeneous information systems.   Information Sciences   329 (2016), 412–433. [6]   E. Bruni, N. K. Tran, and M. Baroni. 2014. Multimodal Distributional Semantics.  Journal of Artificial Intelligence Research   49 (2014), 1–47. [7]   A. Budanitsky and G. Hirst. 2006.   Evaluating WordNet-based measures of semantic distance.   Computational Linguistics   32, 1 (2006), 13–47. [8]   J. Camacho-Collados and M. T. Pilehvar. 2018. From word to sense embeddings: A survey on vector representations of meaning.   Journal of Artificial Intelligence Research   63, 1 (2018), 743–788. [9]   J. Camacho-Collados, M. T. Pilehvar, and R. Navigli. 2016. NASARI: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities.   Artificial Intelligence   240 (2016), 36–64. [10]   D. S. Chaplot and R. Salakhutdinov. 2018.   Knowledge-based word sense disambiguation using topic models. In   Proc. of AAAI Conf. on Artificial Intelligence (AAAI’18) . 5062–5069. [11]   R. L. Cilibrasi and P. M. B. Vitanyi. 2007. The Google Similarity Distance.   IEEE Transactions on Knowledge and Data Engineering   19, 3 (2007), 370–383. [12]   E. A. Correa Jr, A. A. Lopes, and D. R. Amancio. 2018. Word sense disambiguation: A complex network approach.   Information Sciences   442 (2018), 103–113. [13]   L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing Search in Context: The Concept Revisited. In   Proc. of Intl. Conf. on World Wide Web (WWW’01) . 406–414. [14]   J. Gracia and E. Mena. 2008. Web-based Measure of Semantic Relatedness. In   Proc. of Intl. Conf. on Web Information Systems Engineering (WISE’08) . 136–150. [15]   J. Gracia and E. Mena. 2009.   Multiontology Semantic Disambiguation in Unstructured Web Contexts. In   Proc. of Workshop on Collective Knowledge Capturing and Representation (CKCaR’09) at K-CAP’09 . 1–9.  22 QALD is a series of evaluation campaigns on Question Answering over Linked Data: (http://qald.aksw.org). [16]   I. Iacobacci, M. T. Pilehvar, and R. Navigli. 2016. Embeddings for word sense disambiguation: An evaluation study. In   Proc. of Annual Meeting of the Association for Computational Linguistics (ACL’16) . 897–907. [17]   A. Kaplan. 1955. An Experiment Study of Ambiguity and Context.   Mechanical Translation   2, 1 (1955), 39–46. [18]   T. K. Landauer, P. W. Foltz, and D. Laham. 1998. An introduction to latent semantic analysis.   Discourse Processes   25, 2-3 (1998), 259–284. [19]   J. J. Lastra-Díaz, J. Goikoetxea, M. A. H. Taieb, A. García-Serrano, M. B. Aouicha, and E. Agirre. 2019. A reproducible survey on word embeddings and ontology- based methods for word similarity: Linear combinations outperform the state of the art.   Engineering Applications of Artificial Intelligence   85 (2019), 645–665. [20]   J. H. Lau and T. Baldwin. 2016.   An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation. In   Proc. of Workshop on Representation Learning for NLP (Rep4NLP’16) . 78–86. [21]   Q. Le and T. Mikolov. 2014.   Distributed representations of sentences and documents. In   Proc. of Intl. Conf. on Machine Learning (ICML’14) . 1188–1196. [22]   M. Mancini, J. Camacho-Collados, I. Iacobacci, and R. Navigli. 2017. Embedding Words and Senses Together via Joint Knowledge-Enhanced Training. In   Proc. of Conf. on Computational Natural Language Learning (CoNLL’17) . 100–111. [23]   T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In   Advances in Neural Information Processing Systems . 3111–3119. [24]   G. A. Miller and W. G. Charles. 1991. Contextual correlates of semantic similarity.  Language and Cognitive Processes   6, 1 (1991), 1–28. [25]   S. M. Mohammad and G. Hirst. 2012. Distributional measures of semantic distance: A survey.   arXiv preprint arXiv:1203.1858   (2012). [26]   D. Movshovitz-Attias, S. E. Whang, N. Noy, and A. Halevy. 2015. Discovering Subsumption Relationships for Web-Based Ontologies. In   Proc. of Intl. Workshop on Web and Databases (WebDB’15) . 62–69. [27]   R. Navigli. 2009. Word sense disambiguation: A survey.   Comput. Surveys   41, 2 (2009), 1–69. [28]   J. Pennington, R. Socher, and C. Manning. 2014. Glove: Global vectors for word representation. In   Proc. of Conf. on Empirical Methods in Natural Language Processing (EMNLP’14) . 1532–1543. [29]   M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. 2018. Deep contextualized word representations. In   Proc. of Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’18) . 2227–2237. [30]   Y. Pinter, R. Reichart, and I. Szpektor. 2016. Syntactic Parsing of Web Queries with Question Intent. In   Proc. of Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’16) . 670–680. [31]   G. Pirró. 2012. REWOrD: Semantic Relatedness in the Web of Data. In   Proc. of AAAI Conf. on Artificial Intelligence (AAAI’12) . 129–135. [32]   R. S. Roy, S. Agarwal, N. Ganguly, and M. Choudhury. 2016. Syntactic complexity of Web search queries through the lenses of language models, networks and users.  Information Processing & Management   52, 5 (2016), 923–948. [33]   H. Rubenstein and J. B. Goodenough. 1965. Contextual correlates of synonymy.  Commun. ACM   8, 10 (1965), 627–633. [34]   M. Sahlgren. 2008. The distributional hypothesis.   Italian Journal of Disability Studies   20, 1 (2008), 33–53. [35]   R. Socher, J. Bauer, C. Manning, and A. Ng. 2013. Parsing with Compositional Vector Grammars. In   Proc. of Annual Meeting of the Association for Computational  8 Linguistics (ACL’13) . 455–465. [36]   R. Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning, A. Ng, and C. Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In   Proc. of Conf. on Empirical Methods in Natural Language Processing (EMNLP’13) . 1631–1642. [37]   L. Vial, B. Lecouteux, and D. Schwab. 2018.   Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships.   arXiv preprint arXiv:1811.00960   (2018). [38]   D. Yuan, J. Richardson, R. Doherty, C. Evans, and E. Altendorf. 2016.   Semi- supervised Word Sense Disambiguation with Neural Models. In   Proc. of Intl. Conf. on Computational Linguistics (COLING’16) . 1374–1385.  9 