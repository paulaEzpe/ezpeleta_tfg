EJECUCION:
    - para ejecutar el backend:
    dentro de la carpeta backend: python3 server.py 

    - para ejecutar el frontend:
    dentro de la carpeta de frontend: npm start

    - para ejecutar lo de opensearch:
    dentro de la carpeta de opensearch: python3 high_client.py

NOTAS:
primero hacer el frontend con react-bootstrap
luego hacer el backend en python con venv y flask
luego usar mongoDB para intentar meter jsons en la bd 
los 49jsons de la bd se metieron bien pero no iban a caber todos
del resto porque eran en total 50gb sin descomprimir lo q lo haria unos 250gb y mondodb atlas ofrece 512mb.
se hizo una prueba de carga y se comprobo que cabian unos 2600 jsons. nada que ver con todos los que habia que cargar
por tanto se paso a utilizar otra bd. se barajó tambien con cassandra poque tb almacena jsons, pero siendo que a parte de almacenarlos tb hay que indexafrlos para buscar rapidamente entre tanto documento, se paso a elegir opensearch. con docker, ya que es la opcion que recomiendan ellos.
me daba error con el docker el opensearch ultima version de 2.10... asi que busque por internet y era porque no era estabble. use la 1.2.3 y luego me dio un error que se resolvio? con pip install 'urllib3<2'.
problema al almacenar campos por el limite de los index, porque al almacenar 10 documentos, ya son un total de 20000 campos.
ya he arreglado eso y lo que he hecho ha sido serializar el body entero, de manera que el indice queda
(paper_id, documento_serializado_entero). los tiempos que de momento manejo son los siguientes:

tiempo de inserción de la carpeta_08 con 41 elementos, 122,4 MB:
--- 59.84279656410217 seconds ---
--- 778 insertados ---
Además, esto ocupa 182,2 mb en opensearch

segun ese dato, el calculo para el tiempo de insercion de la carpeta con mas documentos, que tiene 1275 elementos y 13.3 GB:
122 ---- 1 minuto
13300 ---- 109 minutos
33600 ----- 300 minutos aprox

para decidir si borrar e campo ref_entries de los jsons, ya que es un campo con el que nunca se va a trabajar y puede merecer la pena 
borrarlo en cuanto a espacio "salvado" en opensearch, se ha extraido el campo de ref_entries de 778 documentos jsons
y se han ido insertando el un archivo de texto. el tamaño de ese archivo es  333202 bytes, que entre los 778 documentos
hace una media de 428.28020565552697 por documento. 

ademas, hay que explicar lo de los autores que a veces en las referencias no salen en su propio campo
sino que pueden aparecer en el nombre de la referencia. mirar ademas si el nombre de los autores tiene algun id o algo (no tienen id, en el campo authors_parsed solo 
aparecen los nombres y apellidos). y por que eliminar el campo de ref_entries?:

    Tamaño total de la base de datos: Si la base de datos es pequeña o moderada, una reducción de 400 bytes por archivo podría no parecer significativa en términos absolutos. Sin embargo, en bases de datos más grandes, esta reducción puede sumar y tener un impacto considerable en el tamaño total de la base de datos.

    Recursos del sistema: Si la base de datos se encuentra en un entorno con recursos limitados, como espacio en disco o ancho de banda, cualquier reducción en el tamaño de los archivos puede ser beneficiosa para optimizar el rendimiento y la eficiencia del sistema.

    Velocidad de acceso y transferencia: Reducir el tamaño de los archivos puede mejorar la velocidad de acceso y transferencia de datos, especialmente en situaciones donde los recursos de red son limitados o el acceso a la base de datos se realiza a través de conexiones lentas.

    Facilidad de mantenimiento: Menos datos pueden significar una menor complejidad y un mantenimiento más fácil de la base de datos. Esto puede ser particularmente útil si necesitas realizar copias de seguridad regulares de la base de datos o si estás gestionando la base de datos en un entorno de almacenamiento en la nube donde el costo está relacionado con la cantidad de datos almacenados.

se ha decidido que se indexará por autores, paper_id, disciplina, categoría y título

Data inserted successfully.
--- 123.13113975524902 seconds ---
--- 1327 insertados ---

Data inserted successfully.
--- 178.07762551307678 seconds ---
--- 1596 insertados ---


estaba teniendo problemas al buscar titulos porque buscaba un titulo que tecnicawmtne no estaba insertado y me salian muchos id, pero lo que pasaba
era que estaba usando match en vez de term, que mira si coincide una parte de la cadena.

--- 1639.703936100006 seconds ---
--- 15827 insertados ---

Data inserted successfully.
--- 6170.454500436783 seconds ---
--- 58385 insertados --- esto la carpeta de 11.9 gb (100 minutos, casi 2h)

Data inserted successfully.
--- 10190.409915924072 seconds ---
--- 65757 insertados ---



--------------------------------------------------------------------------------
 18/04: escribir en la memoria:
 implementacion y diseño del frontend
 implementacion y diseño del backend
    implementacion de los distintos modulos???
    dentro del de texto, los algoritmos para comprobar si una cita esta contenida en un parrafo y la comparacion de tiempos de estos???
    toda la parte que estoy haciendo ahora de entrenar el modelo para saber a partir de una cita y una referencia si te vale con leerte el titulo y abstract o no¿¿¿

all words: https://www.academicvocabulary.info/download.asp


se han usado estos modulos:
pip3 install transformers --break-system-packages
pip3 install torch --break-system-packages
pip3 install sentence_transformers --break-system-packages


El tiempo de entrenamiento de un modelo de Word2Vec no escala de manera lineal con el aumento de la dimensionalidad del vector. Sin embargo, hay algunas consideraciones generales que pueden ayudarte a estimar el tiempo de entrenamiento.

    Complejidad Computacional: La complejidad computacional de entrenar un modelo de Word2Vec aumenta con la dimensionalidad del vector. Específicamente, el tiempo de entrenamiento puede crecer de manera cuadrática con la dimensionalidad en el peor de los casos, aunque esto puede variar dependiendo de la implementación y las optimizaciones específicas del algoritmo.

    Recursos Computacionales: La cantidad de recursos computacionales disponibles, como la CPU, la memoria y la capacidad de paralelización, también afecta significativamente el tiempo de entrenamiento.

    Tamaño del Corpus: El tamaño del corpus de entrenamiento también es un factor importante. Un corpus más grande requerirá más tiempo de procesamiento.

Estimación Aproximada

Dado que mencionaste que entrenar un modelo con una dimensionalidad de 50 tomó 89 horas, y asumiendo una relación cuadrática simplificada (aunque en la práctica puede variar), puedes usar la siguiente fórmula para una estimación aproximada:

Tiempo nuevo≈Tiempo original×(Nueva dimensionalidadDimensionalidad original)2
Tiempo nuevo≈Tiempo original×(Dimensionalidad originalNueva dimensionalidad​)2

En tu caso:

Tiempo nuevo≈89 horas×(20050)2
Tiempo nuevo≈89 horas×(50200​)2
Tiempo nuevo≈89 horas×(4)2
Tiempo nuevo≈89 horas×16
Tiempo nuevo≈1424 horas
Consideraciones Finales

    Optimización: Hay optimizaciones posibles que pueden reducir significativamente el tiempo de entrenamiento, como el uso de técnicas de sampling negativo, ventanas de contexto más pequeñas, o hardware más potente.

    Paralelización: Asegúrate de usar la capacidad de paralelización de Word2Vec (como el parámetro workers en Gensim) para aprovechar múltiples núcleos de CPU.

    Validación Práctica: La fórmula proporcionada es una estimación teórica. En la práctica, te recomiendo realizar una prueba con una dimensionalidad intermedia para obtener una mejor estimación basada en tu configuración específica.


---------------------------------

MODELO GOOGLE:

(w,dim): (3000000,300)
Palabra desconocida: chapter,
Palabra desconocida: and
Palabra desconocida: Haskell.
Palabra desconocida: types,
Palabra desconocida: a
Palabra desconocida: types.
Palabra desconocida: Next,
Palabra desconocida: to
Palabra desconocida: and
Palabra desconocida: instances.
Palabra desconocida: Finally,
Palabra desconocida: a
Palabra desconocida: and
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 11
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 51
Palabras desconocidas en el artículo: 13
Palabras totales en el artículo: 51
Porcentaje de desconocidas en la cita: 0.244444
Porcentaje de desconocidas en el artículo: 0.254902
Palabra desconocida: to
Palabra desconocida: and
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 11
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 11
Palabras desconocidas en el artículo: 2
Palabras totales en el artículo: 11
Porcentaje de desconocidas en la cita: 0.244444
Porcentaje de desconocidas en el artículo: 0.181818
Palabra desconocida: to
Palabra desconocida: and
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 11
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 10
Palabras desconocidas en el artículo: 2
Palabras totales en el artículo: 10
Porcentaje de desconocidas en la cita: 0.244444
Porcentaje de desconocidas en el artículo: 0.200000
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 11
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 10
Palabras desconocidas en el artículo: 0
Palabras totales en el artículo: 10
Porcentaje de desconocidas en la cita: 0.244444
Porcentaje de desconocidas en el artículo: 0.000000
Palabra desconocida: of
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 11
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 9
Palabras desconocidas en el artículo: 1
Palabras totales en el artículo: 9
Porcentaje de desconocidas en la cita: 0.244444
Porcentaje de desconocidas en el artículo: 0.111111
Palabra desconocida: of
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 11
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 10
Palabras desconocidas en el artículo: 1
Palabras totales en el artículo: 10
Porcentaje de desconocidas en la cita: 0.244444
Porcentaje de desconocidas en el artículo: 0.100000
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 11
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 3
Palabras desconocidas en el artículo: 0
Palabras totales en el artículo: 3
Porcentaje de desconocidas en la cita: 0.244444
Porcentaje de desconocidas en el artículo: 0.000000
(0,0): 0.958956
(0,3): 0.864859
(0,1): 0.757558
(0,2): 0.748385
(0,4): 0.678292
(0,5): 0.667337
(0,6): 0.504121



MODELO PAULA SIN WORKERS:

(w,dim): (771416,50)
Palabra desconocida: In
Palabra desconocida: chapter,
Palabra desconocida: Haskell.
Palabra desconocida: We
Palabra desconocida: types,
Palabra desconocida: types.
Palabra desconocida: Next,
Palabra desconocida: instances.
Palabra desconocida: Finally,
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 6
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 51
Palabras desconocidas en el artículo: 9
Palabras totales en el artículo: 51
Porcentaje de desconocidas en la cita: 0.133333
Porcentaje de desconocidas en el artículo: 0.176471
Palabra desconocida: He
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 6
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 11
Palabras desconocidas en el artículo: 1
Palabras totales en el artículo: 11
Porcentaje de desconocidas en la cita: 0.133333
Porcentaje de desconocidas en el artículo: 0.090909
Palabra desconocida: They
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 6
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 10
Palabras desconocidas en el artículo: 1
Palabras totales en el artículo: 10
Porcentaje de desconocidas en la cita: 0.133333
Porcentaje de desconocidas en el artículo: 0.100000
Palabra desconocida: In
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 6
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 10
Palabras desconocidas en el artículo: 1
Palabras totales en el artículo: 10
Porcentaje de desconocidas en la cita: 0.133333
Porcentaje de desconocidas en el artículo: 0.100000
Palabra desconocida: It
Palabra desconocida: Haskell
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 6
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 9
Palabras desconocidas en el artículo: 2
Palabras totales en el artículo: 9
Porcentaje de desconocidas en la cita: 0.133333
Porcentaje de desconocidas en el artículo: 0.222222
Palabra desconocida: It
Palabra desconocida: Haskell
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 6
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 10
Palabras desconocidas en el artículo: 2
Palabras totales en el artículo: 10
Porcentaje de desconocidas en la cita: 0.133333
Porcentaje de desconocidas en el artículo: 0.200000
Palabra desconocida: This
Palabras conocidas en la cita: 45
Palabras desconocidas en la cita: 6
Palabras totales en la cita: 45
Palabras conocidas en el artículo: 3
Palabras desconocidas en el artículo: 1
Palabras totales en el artículo: 3
Porcentaje de desconocidas en la cita: 0.133333
Porcentaje de desconocidas en el artículo: 0.333333
(0,0): 0.994425
(0,3): 0.972769
(0,1): 0.931238
(0,5): 0.925659
(0,4): 0.924380
(0,2): 0.914397
(0,6): 0.733072



MODELO FASTTEXT:

Dimensión del modelo: 300
Palabras conocidas en la cita: 46
Palabras totales en la cita: 46
Palabras conocidas en el artículo: 51
Palabras totales en el artículo: 51
Porcentaje de desconocidas en la cita: 0.000000
Porcentaje de desconocidas en el artículo: 0.000000
Palabras conocidas en la cita: 46
Palabras totales en la cita: 46
Palabras conocidas en el artículo: 11
Palabras totales en el artículo: 11
Porcentaje de desconocidas en la cita: 0.000000
Porcentaje de desconocidas en el artículo: 0.000000
Palabras conocidas en la cita: 46
Palabras totales en la cita: 46
Palabras conocidas en el artículo: 10
Palabras totales en el artículo: 10
Porcentaje de desconocidas en la cita: 0.000000
Porcentaje de desconocidas en el artículo: 0.000000
Palabras conocidas en la cita: 46
Palabras totales en la cita: 46
Palabras conocidas en el artículo: 10
Palabras totales en el artículo: 10
Porcentaje de desconocidas en la cita: 0.000000
Porcentaje de desconocidas en el artículo: 0.000000
Palabras conocidas en la cita: 46
Palabras totales en la cita: 46
Palabras conocidas en el artículo: 9
Palabras totales en el artículo: 9
Porcentaje de desconocidas en la cita: 0.000000
Porcentaje de desconocidas en el artículo: 0.000000
Palabras conocidas en la cita: 46
Palabras totales en la cita: 46
Palabras conocidas en el artículo: 10
Palabras totales en el artículo: 10
Porcentaje de desconocidas en la cita: 0.000000
Porcentaje de desconocidas en el artículo: 0.000000
Palabras conocidas en la cita: 46
Palabras totales en la cita: 46
Palabras conocidas en el artículo: 3
Palabras totales en el artículo: 3
Porcentaje de desconocidas en la cita: 0.000000
Porcentaje de desconocidas en el artículo: 0.000000
(0,0): 0.982578
(0,3): 0.883266
(0,1): 0.823928
(0,2): 0.821448
(0,5): 0.816599
(0,4): 0.804070
(0,6): 0.594541


---------------------------------

SE VA A HACER EL ENTRENAMIENTO CON VARIOS WORKERS EN PARALELO A PARTIR DE UN TXT



resumen y objetivos como subseccion dentro de la introduccion
quitar disk storage

unir contexto tecnológico y herramientas utilizadas: hablar de todo lo de fasttext, hablar de ontext otecnologico de modelos de lenguaje,
busquedas. hablar de que es un modelo de lenguaje, de que hemos usando gensim......


CALCULOS DE OOV DE CADA MODELO:
fasttext:
2024-06-09 22:13:30,042 - INFO - Palabras OOV en ../datos/txt_reunidos_2_4_limpio.txt: 3296488
2024-06-09 22:13:30,042 - INFO - Procesadas 67588747 palabras de ../datos/txt_reunidos_2_4_limpio.txt en 11.55 segundos
2024-06-09 22:13:56,830 - INFO - Palabras OOV en ../datos/txt_reunidos_18_19_20_limpio.txt: 7235363
2024-06-09 22:13:56,830 - INFO - Procesadas 154636595 palabras de ../datos/txt_reunidos_18_19_20_limpio.txt en 26.79 segundos
2024-06-09 22:14:51,815 - INFO - Palabras OOV en ../datos/txt_reunidos_21_limpio.txt: 13459129
2024-06-09 22:14:51,815 - INFO - Procesadas 323264834 palabras de ../datos/txt_reunidos_21_limpio.txt en 54.99 segundos
2024-06-09 22:15:54,164 - INFO - Palabras OOV en ../datos/txt_reunidos_22_limpio.txt: 15097768
2024-06-09 22:15:54,164 - INFO - Procesadas 367524793 palabras de ../datos/txt_reunidos_22_limpio.txt en 62.35 segundos

google:
2024-06-09 21:33:04,099 - INFO - Palabras OOV en ../datos/txt_reunidos_2_4_limpio.txt: 11525280
2024-06-09 21:33:04,099 - INFO - Procesadas 67588747 palabras de ../datos/txt_reunidos_2_4_limpio.txt en 11.27 segundos
2024-06-09 21:33:31,758 - INFO - Palabras OOV en ../datos/txt_reunidos_18_19_20_limpio.txt: 25839695
2024-06-09 21:33:31,758 - INFO - Procesadas 154636595 palabras de ../datos/txt_reunidos_18_19_20_limpio.txt en 27.66 segundos
2024-06-09 21:34:32,034 - INFO - Palabras OOV en ../datos/txt_reunidos_21_limpio.txt: 51616057
2024-06-09 21:34:32,035 - INFO - Procesadas 323264834 palabras de ../datos/txt_reunidos_21_limpio.txt en 60.28 segundos
2024-06-09 21:35:42,449 - INFO - Palabras OOV en ../datos/txt_reunidos_22_limpio.txt: 58348127
2024-06-09 21:35:42,449 - INFO - Procesadas 367524793 palabras de ../datos/txt_reunidos_22_limpio.txt en 70.41 segundos

paula:
2024-06-09 22:07:02,151 - INFO - Palabras OOV en ../datos/txt_reunidos_2_4_limpio.txt: 367749
2024-06-09 22:07:02,151 - INFO - Procesadas 67588747 palabras de ../datos/txt_reunidos_2_4_limpio.txt en 11.32 segundos
2024-06-09 22:07:30,048 - INFO - Palabras OOV en ../datos/txt_reunidos_18_19_20_limpio.txt: 775366
2024-06-09 22:07:30,048 - INFO - Procesadas 154636595 palabras de ../datos/txt_reunidos_18_19_20_limpio.txt en 27.90 segundos
2024-06-09 22:08:24,963 - INFO - Palabras OOV en ../datos/txt_reunidos_21_limpio.txt: 1452619
2024-06-09 22:08:24,963 - INFO - Procesadas 323264834 palabras de ../datos/txt_reunidos_21_limpio.txt en 54.91 segundos
2024-06-09 22:09:26,147 - INFO - Palabras OOV en ../datos/txt_reunidos_22_limpio.txt: 1648672
2024-06-09 22:09:26,147 - INFO - Procesadas 367524793 palabras de ../datos/txt_reunidos_22_limpio.txt en 61.18 segundos

BERT:

BERT + transformers:






-----------------------------------------------------------------


Autor(es): Bornmann, L., & Daniel, H.-D.

    Título: "What do citation counts measure? A review of studies on citing behavior."
    Revista: Journal of Documentation
    Año: 2008
    DOI: 10.1108/00220410810899796
    Resumen: Este artículo revisa estudios sobre el comportamiento de citación, discutiendo qué miden los recuentos de citas y cómo se utilizan en la evaluación de la investigación.

Autor(es): Small, H.

    Título: "Co-citation in the scientific literature: A new measure of the relationship between two documents."
    Revista: Journal of the American Society for Information Science
    Año: 1973
    DOI: 10.1002/asi.4630240406
    Resumen: Introduce el análisis de co-citación como una herramienta para medir la relación entre dos documentos científicos, lo que es relevante para la verificación y análisis de citas.

Autor(es): Gipp, B., & Beel, J.

    Título: "Citation Proximity Analysis (CPA) - A new approach for identifying related work based on co-citation analysis."
    Revista: Proceedings of the 12th International Conference on Scientometrics and Informetrics
    Año: 2009
    Resumen: Presenta un nuevo enfoque para identificar trabajos relacionados basándose en el análisis de proximidad de citas, útil para la recomendación y verificación de citas.

Autor(es): Peroni, S., & Shotton, D.

    Título: "FaBiO and CiTO: ontologies for describing bibliographic resources and citations."
    Revista: Journal of Web Semantics
    Año: 2012
    DOI: 10.1016/j.websem.2012.08.001
    Resumen: Discute las ontologías FaBiO y CiTO para describir recursos bibliográficos y citas, proporcionando un marco para el análisis detallado de citas y su verificación.

Autor(es): Teufel, S., Siddharthan, A., & Tidhar, D.

    Título: "An annotation scheme for citation function."
    Revista: Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue
    Año: 2006
    Resumen: Propone un esquema de anotación para la función de las citas, lo cual es relevante para entender el contexto y la intención detrás de las citas en la literatura científica.